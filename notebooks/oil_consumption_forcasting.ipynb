{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:01.975300400Z",
     "start_time": "2024-10-12T15:02:01.896950800Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:01.991417800Z",
     "start_time": "2024-10-12T15:02:01.912920600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:01.991417800Z",
     "start_time": "2024-10-12T15:02:01.921682900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "plt.rcParams['font.family'] = 'STIXGeneral'\n",
    "plt.rcParams['font.size'] = 12\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load cleaned data\n",
    "Data is preprocessed in the [data preprocessing](./create_dataset.ipynb) notebook. This includes concatenating the data, removing outliers, missing values, and irrelevant columns and generating relevant features based on given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:02.053933500Z",
     "start_time": "2024-10-12T15:02:01.922188900Z"
    }
   },
   "outputs": [],
   "source": [
    "data_per_day = pd.read_pickle('../data/processed/data_one_day_clean.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:02.053933500Z",
     "start_time": "2024-10-12T15:02:01.931367Z"
    }
   },
   "outputs": [],
   "source": [
    "data_per_day.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:02.053933500Z",
     "start_time": "2024-10-12T15:02:01.944897800Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert date columns from object to datetime\n",
    "date_cols = ['Zeitstempel', 'Sicherheitsbestand wird erreicht am', 'Meldebestand wird erreicht am']\n",
    "for col in date_cols:\n",
    "    data_per_day[col] = pd.to_datetime(data_per_day[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:02.053933500Z",
     "start_time": "2024-10-12T15:02:01.954041200Z"
    }
   },
   "outputs": [],
   "source": [
    "data_per_day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build target variable \"Verbrauch\" per day\n",
    "Difference of the \"Füllstand\" the current day to the next day. This is the oil consumption per day. Implementation also see [data preprocessing](./create_dataset.ipynb), calculate \"Füllstand\" difference from day before to current day, then shift by one day to the past, since this difference is the consumption of the day before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:02.053933500Z",
     "start_time": "2024-10-12T15:02:01.975300400Z"
    }
   },
   "outputs": [],
   "source": [
    "#!IMPORTANT! Shift the consumption by one day to the future, since on day x the \"Füllstand\" of day x+1 is not known, necessary for calculating the consumption of day x\n",
    "for id in data_per_day[\"Tank-ID\"].unique():\n",
    "    temp_df = pd.DataFrame(data_per_day[data_per_day[\"Tank-ID\"] == id])\n",
    "    temp_df[\"Verbrauch\"] = temp_df[\"Füllstand\"].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat matching historical weather data\n",
    "We implemented a wrapper to gain historical and forecast weather data per day using an open source API, see in [weather data](../src/api/weather.py) notebook. This then can be easily concatenated as an external feature to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:02.836492800Z",
     "start_time": "2024-10-12T15:02:01.975300400Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.api import WeatherAPI\n",
    "\n",
    "weather_api = WeatherAPI()\n",
    "\n",
    "merged_dfs = []\n",
    "for id in data_per_day[\"Tank-ID\"].unique():\n",
    "    temp_df = data_per_day[data_per_day[\"Tank-ID\"] == id]\n",
    "\n",
    "    # get attributes\n",
    "    latitude = temp_df[\"Längengrad\"].iloc[0]\n",
    "    longitude = temp_df[\"Breitengrad\"].iloc[0]\n",
    "    if temp_df[\"Zeitstempel\"].dtypes == 'object':\n",
    "        temp_df[\"Zeitstempel\"] = pd.to_datetime(temp_df[\"Zeitstempel\"])\n",
    "    start_date = temp_df[\"Zeitstempel\"].min().strftime(\"%Y-%m-%d\")\n",
    "    end_date = temp_df[\"Zeitstempel\"].max().strftime(\"%Y-%m-%d\")\n",
    "    print(\"Start date:\", start_date, \"End date:\", end_date, \"Day difference:\", (temp_df[\"Zeitstempel\"].max() - temp_df[\"Zeitstempel\"].min()).days)\n",
    "\n",
    "    # get matching weather data\n",
    "    weather_data = weather_api.get_data(latitude, longitude, start_date, end_date)\n",
    "\n",
    "    # remove timezone information\n",
    "    weather_data['date'] = weather_data['date'].dt.tz_localize(None)\n",
    "    weather_data = weather_data.rename(columns={'date': 'Zeitstempel'})\n",
    "\n",
    "    # join\n",
    "    print(\"Data shape before merge:\", temp_df.shape)\n",
    "    temp_df = temp_df.merge(weather_data, on=['Zeitstempel'], how='left')\n",
    "    print(\"Data shape after merge:\", temp_df.shape)\n",
    "    display(temp_df.head(5))\n",
    "\n",
    "    # append\n",
    "    merged_dfs.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:02.836492800Z",
     "start_time": "2024-10-12T15:02:02.672332700Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_data = pd.concat(merged_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:02.836492800Z",
     "start_time": "2024-10-12T15:02:02.711081Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Data shape before merging weatehr data:\", data_per_day.shape)\n",
    "print(\"Data shape after merging weather data:\", merged_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:02.836492800Z",
     "start_time": "2024-10-12T15:02:02.711081Z"
    }
   },
   "outputs": [],
   "source": [
    "# set index to \"Zeitstempel\"\n",
    "df = merged_data.set_index('Zeitstempel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Investigate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:03.150664500Z",
     "start_time": "2024-10-12T15:02:02.711081Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot \"Füllstand\" time series based on tank ID\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for tank_id in df[\"Tank-ID\"].unique():\n",
    "    df[df[\"Tank-ID\"] == tank_id][\"Füllstand\"].plot(ax=ax, label=f'Tank {tank_id}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Füllstand')\n",
    "plt.title('\"Füllstand\" over time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since the Tank ID 5 has a different pattern than the other tanks, and the data is not complete, we will drop this tank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:03.150664500Z",
     "start_time": "2024-10-12T15:02:03.016656200Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop irrelevant columns\n",
    "cols_to_drop = [\"Sicherheitsbestand wird erreicht am\", \"Meldebestand wird erreicht am\"] # \"Füllstand\"\n",
    "df = df.drop(cols_to_drop, axis=1)\n",
    "# drop ID 5\n",
    "df = df[df['Tank-ID'] != 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:03.584147300Z",
     "start_time": "2024-10-12T15:02:03.025247500Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot target \"Verbrauch\" time series based on tank ID\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "for tank_id in df[\"Tank-ID\"].unique():\n",
    "    df[df[\"Tank-ID\"] == tank_id][\"Verbrauch\"].plot(ax=axes[0], label=f'Tank {tank_id}')\n",
    "    df[df[\"Tank-ID\"] == tank_id][\"Verbrauch smoothed\"].plot(ax=axes[1], label=f'Tank {tank_id}')\n",
    "\n",
    "axes[0].set_title('Consumption over time')\n",
    "axes[1].set_title('Consumption over time (smoothed)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Verbrauch')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since outliers above 0 make no sense and also in our implemented smoothing functionality it is detected as outliers we will set a maximum value of 0 for the consumption. Logically also one can not consume oil and the tank gets filled up by consuming it. This is a clear indicator for a wrong measurement. In the plot above we can see that the consumption is mostly below 0, where a value of -10 indicates a consumption of 10 liters, the negative sign is due to the resulting decline in the tank by consumption. Thus the 0 value indicates a full no consumption and thus no decline in the tank. For better understanding we will further take the absolute value of the consumption. after correcting the outliers due to sensor errors.\n",
    "--> 1. Set consumption to 0 if above 0\n",
    "--> 2. Take absolute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:03.590166600Z",
     "start_time": "2024-10-12T15:02:03.584147300Z"
    }
   },
   "outputs": [],
   "source": [
    "# correct outliers\n",
    "df.loc[df['Verbrauch'] > 0, 'Verbrauch'] = 0.0\n",
    "# take absolute values\n",
    "df['Verbrauch'] = df['Verbrauch'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:03.936833400Z",
     "start_time": "2024-10-12T15:02:03.584147300Z"
    }
   },
   "outputs": [],
   "source": [
    "# check if outliers are corrected\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "for tank_id in df[\"Tank-ID\"].unique():\n",
    "    df[df[\"Tank-ID\"] == tank_id][\"Verbrauch\"].plot(ax=ax, label=f'Tank {tank_id}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Verbrauch')\n",
    "plt.title('Consumption over time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Prepare Train and Test Data\n",
    "For forcasting following steps are necessary:\n",
    "1. Split data into train and test data\n",
    "2. Normalize data\n",
    "3. (Create sequences of data)\n",
    "5. Split data into X and y\n",
    "8. Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:03.936833400Z",
     "start_time": "2024-10-12T15:02:03.826530100Z"
    }
   },
   "outputs": [],
   "source": [
    "# split data into train and test data, based on tank ID\n",
    "train_data = df[df['Tank-ID'] != 2]\n",
    "test_data = df[df['Tank-ID'] == 2]\n",
    "\n",
    "# get x and y values\n",
    "X_train = train_data.drop('Verbrauch', axis=1).values\n",
    "y_train = train_data['Verbrauch'].values\n",
    "X_test = test_data.drop('Verbrauch', axis=1).values\n",
    "y_test = test_data['Verbrauch'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:02:16.303186Z",
     "start_time": "2024-10-12T15:02:16.271251200Z"
    }
   },
   "outputs": [],
   "source": [
    "# save data\n",
    "np.save('../data/processed/X_train.npy', X_train)\n",
    "np.save('../data/processed/y_train.npy', y_train)\n",
    "np.save('../data/processed/X_test.npy', X_test)\n",
    "np.save('../data/processed/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:52:08.396030500Z",
     "start_time": "2024-10-12T13:52:08.364530600Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:52:09.993729Z",
     "start_time": "2024-10-12T13:52:09.981902100Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Feature Selection\n",
    "* via correlation\n",
    "* via feature importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Option 1: ARIMA model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Reasoning:\n",
    "* As seen in the plot of the target variable \"Verbrauch\" over time above, we can see that the oil consumption has a clear trend and seasonality:\n",
    "* Peaks in winter (high consumption for heating), dips in summer (lower demand), and rises again in autumn. The pattern is cyclical, reflecting higher oil use in colder months and lower in warmer months.\n",
    "\n",
    "This suggests that the time series is not stationary and will require differencing to make it stationary, at least a difference order of 1.\n",
    "\n",
    "--> This is a good indicator for using an ARIMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting autocorrelation and partial autocorrelation for an extensive set of time series lags, hue as the id\n",
    "# Function to calculate autocorrelation\n",
    "def autocorr_plot(df, lags=30):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Loop over each unique ID and plot autocorrelation for each\n",
    "    for id_value in df['id'].unique():\n",
    "        subset = df[df['id'] == id_value]\n",
    "        plot_acf(subset['value'], lags=lags, alpha=0.05, title=f'Autocorrelation for ID {id_value}', ax=plt.gca())\n",
    "\n",
    "    plt.legend(df['id'].unique(), title='ID')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your dataframe\n",
    "autocorr_plot(df, lags=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:52:13.004624500Z",
     "start_time": "2024-10-12T13:52:12.907194500Z"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# fit model\n",
    "model = ARIMA(y_train, order=(5,1,0))\n",
    "\n",
    "model_fit = model.fit()\n",
    "\n",
    "# make prediction\n",
    "yhat = model_fit.forecast(steps=len(y_test))\n",
    "\n",
    "# evaluate forecast (Best: RMSE = 0, Worst: RMSE = 1)\n",
    "rmse = sqrt(mean_squared_error(y_test, yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Prophet model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: Simple ML model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bfh24",
   "language": "python",
   "name": "bfh24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
